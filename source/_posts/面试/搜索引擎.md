---
title: 搜索引擎多连炮
date: 2020-12-28
categories:
    - 面试
tags:
    - 搜索引擎
---

#### es分布式架构的原理？ 

es 设计的概念就是分布式搜索引擎，底层其实还是基于Lucene的

核心的思想就是在多台机器上启动多个es进程实例，组成了一个es集群。es中存储数据的基本单位是索引，这个索引可以拆分出多个shard
每个shard存储部分数据 。index -> type -> mapping -> document -> field

shard 又分为 primary shard 和 replica shard 并且分不到集群中的不同机器中 。 数据只能写入primary shard ，然后primary shard 
再将数据同步到replica shard上去， es客户端获取数据既可以从primary shard 又可以从 replica shard 读


![es读写数据原理](/images/面试/es读写数据原理.png)

#### es写入数据的工作原理？
1. 客户端选择一个node发送请求过去，这个node就是协调节点（coordinating node）
2. 协调节点对document进行路由(根据docId hash)，将请求转发给对应的node(有primary shard)
3. 实际上是由node的 primary shard 处理请求，然后将数据同步到其他node(有replica shard)
4. 协调节点发现primary shard 和 所有 replica node都搞定之后，就返回响应结果给客户端

> 数据如何写入磁盘：
1. primary shard 先将数据写入buffer（在buffer里面的数据是搜索不到的），同时将数据写入translog日志文件
2. 如果buffer快写满了或者到了一定的时间（默认1秒），就会将buffer数据**refresh**到os cache中（在os cache中的数据可以被搜索到）
    * **es是准实时的**，写入的数据1秒以后才能被看到，可以调用es API手动**refresh**，将buffer中的数据刷入os cache中，让数据可以立马被搜索到
3. 当translog不断变大，达到一定阈值就会触发**flush/commit**操作
    * primary shard 写入一个commit point到磁盘文件，里面标识了这个commit point对应的所有segment file
    * 强行将os cache中目前所有的数据都fsync 到磁盘中去
    * 将现有的translog清空，然后重新启用一个translog，此时commit操作完成
        * 默认每隔30分钟会自动执行一次flush，我们也可以调用es API手动flush
4. translog其实也是先写入os cache的，每隔5秒刷一次到磁盘中去
    * 最多丢失5秒的数据，可以设置成每次写操作必须直接fsync到磁盘，但是性能较差


#### es读取数据的工作原理？
1. 客户端发送请求到任意一个node, 成为协调节点
2. 协调节点对document进行路由，将请求转发到对应的node,此时会使用round-robin(随机轮询算法)，在primary shard以及所有replica
shard 中随机选择一个，让读请求做负载均衡
3. 接收到请求的node返回document给协调节点
4. 协调节点返回document给客户端

#### es搜索数据的过程？
1. 客户端发送请求到一个协调节点
2. 协调节点将请求转发到所有的shard（primary或replica）
3. 每个shard将自己的搜索结果（其实就是一些docId）， 返回给协调节点，然后由协调节点进行数据的合并、排序、分页等操作
4. 最后协调节点根据docId去各个节点上去拉取实际的document数据返回给客户端

#### es数据量很大的情况（数10亿级别），如何提高查询效率？
1. 性能优化的杀手锏 -- filesystem cache
   * es设计时只存储需要搜索的字段
   * es + hbase / mysql
2. 数据预热
   * 定时任务手动搜索热点数据
3. 冷热分离
   * 将热点数据和冷数据分索引存储
4. document模型设计
   * 涉及多表提前关联好需要查询的字段
5. es分页
    * 不允许深度搜索，默认深度分页性能很差
    * 用scroll api ，顺序分页，类似于微博一页页的拉取，不允许跨页
        * scroll会一次性给你生成所有数据的一个快照，然后每次翻页就是通过游标移动，获取下一页数据

#### es生产集群的部署架构是什么？每个索引的数据量大概有多少？每个索引大概有多少分片？
1. es生产集群我们部署了5台机器，每台机器是6核64G的，集群总内存是320G
2. 我们es集群的日增数据量大概是2000万条，每天日增数据量大概是500MB,每月增量数据大概是6亿/15G。目前系统已经运行了几个月
，现在es集群里数据总量大概是100G左右
3. 目前线上有5个索引（这个结合业务来），每个索引的数据量大概是20G,所以这个数据量之内，我们索引分配的是8个shard,比默认的5个shard
多了3个shard
